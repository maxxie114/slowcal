{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SF Business Risk Prediction Engine\n",
        "\n",
        "## Complete End-to-End Machine Learning Pipeline\n",
        "\n",
        "This notebook implements a comprehensive machine learning pipeline for predicting small business closure risk using SF.gov open data.\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "1. **Setup** - Imports, configuration, and data download\n",
        "2. **Data Loading & Exploration** - Load and explore business data\n",
        "3. **Data Cleaning** - Clean and standardize datasets\n",
        "4. **Feature Engineering** - Create predictive features\n",
        "5. **Model Preparation** - Prepare training data\n",
        "6. **Model Training** - Train Gradient Boosting model\n",
        "7. **Visualizations** - Model evaluation visualizations\n",
        "8. **Model Deployment** - Save and test model\n",
        "9. **Real-World Application** - Batch screening and alerts\n",
        "10. **Summary & Next Steps** - Key findings and improvements\n",
        "\n",
        "---\n",
        "\n",
        "**Data Sources:** SF.gov Open Data API  \n",
        "**Model:** Gradient Boosting Classifier  \n",
        "**Target:** Business closure prediction (binary classification)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Imports\n",
        "print(\"üì¶ IMPORTING LIBRARIES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, roc_auc_score,\n",
        "    roc_curve, precision_recall_curve, average_precision_score,\n",
        "    precision_score, recall_score, f1_score\n",
        ")\n",
        "import joblib\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(f\"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Configuration\n",
        "print(\"‚öôÔ∏è SETTING UP CONFIGURATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Project paths\n",
        "PROJECT_ROOT = Path().parent.parent\n",
        "DATA_DIR = PROJECT_ROOT / \"data\"\n",
        "RAW_DATA_DIR = DATA_DIR / \"raw\"\n",
        "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
        "MODELS_DIR = DATA_DIR / \"models\"\n",
        "\n",
        "# Create directories\n",
        "for dir_path in [DATA_DIR, RAW_DATA_DIR, PROCESSED_DATA_DIR, MODELS_DIR]:\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"üìÅ {dir_path}\")\n",
        "\n",
        "# SF.gov API endpoints (as specified)\n",
        "DATASETS = {\n",
        "    'registered_business': 'https://data.sfgov.org/resource/g8m3-pdis.json',\n",
        "    'building_permits': 'https://data.sfgov.org/resource/i98e-djp9.json',\n",
        "    'code_violations': 'https://data.sfgov.org/resource/nbtm-fbw5.json'\n",
        "}\n",
        "\n",
        "# Model configuration\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "print(f\"\\n‚úÖ Configuration complete\")\n",
        "print(f\"üìä Datasets: {len(DATASETS)}\")\n",
        "print(f\"üé≤ Random state: {RANDOM_STATE}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 3: Download Data Functions\n",
        "print(\"üåê SETTING UP DATA DOWNLOAD FUNCTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "import requests\n",
        "import json\n",
        "from typing import Optional\n",
        "\n",
        "def download_sf_data(url: str, output_path: Optional[Path] = None, limit: int = 50000) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Download data from SF.gov Open Data API\n",
        "    \n",
        "    Args:\n",
        "        url: Full API endpoint URL\n",
        "        output_path: Optional path to save raw JSON\n",
        "        limit: Maximum records to download\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with downloaded data\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"$limit\": limit,\n",
        "        \"$order\": \"record_id DESC\"\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        print(f\"  ‚¨áÔ∏è Downloading from {url}...\")\n",
        "        response = requests.get(url, params=params, timeout=60)\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        data = response.json()\n",
        "        \n",
        "        if output_path:\n",
        "            output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            with open(output_path, 'w') as f:\n",
        "                json.dump(data, f, indent=2)\n",
        "            print(f\"  üíæ Saved to {output_path}\")\n",
        "        \n",
        "        df = pd.DataFrame(data)\n",
        "        print(f\"  ‚úÖ Downloaded {len(df):,} records\")\n",
        "        return df\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "print(\"‚úÖ Download functions ready!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 4: Load Business Data\n",
        "print(\"üìä LOADING BUSINESS REGISTRY DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Download or load from file\n",
        "business_file = RAW_DATA_DIR / 'registered_business.csv'\n",
        "if business_file.exists():\n",
        "    print(f\"üìÇ Loading from existing file: {business_file}\")\n",
        "    business_df = pd.read_csv(business_file)\n",
        "else:\n",
        "    print(\"üåê Downloading from SF.gov API...\")\n",
        "    business_df = download_sf_data(\n",
        "        DATASETS['registered_business'],\n",
        "        output_path=RAW_DATA_DIR / 'registered_business.json'\n",
        "    )\n",
        "    # Save as CSV for faster loading next time\n",
        "    if len(business_df) > 0:\n",
        "        business_df.to_csv(business_file, index=False)\n",
        "        print(f\"üíæ Saved to {business_file}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Loaded {len(business_df):,} business records\")\n",
        "print(f\"üìã Columns: {len(business_df.columns)}\")\n",
        "print(f\"\\nColumn names:\")\n",
        "print(list(business_df.columns)[:10], \"...\" if len(business_df.columns) > 10 else \"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 5: Data Exploration\n",
        "print(\"üîç EXPLORING BUSINESS DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"üìê Dataset Shape: {business_df.shape[0]:,} rows √ó {business_df.shape[1]} columns\")\n",
        "print(f\"\\nüìä Data Info:\")\n",
        "business_df.info()\n",
        "\n",
        "print(f\"\\nüìà Summary Statistics:\")\n",
        "display(business_df.describe())\n",
        "\n",
        "print(f\"\\nüëÄ Sample Data (first 5 rows):\")\n",
        "display(business_df.head())\n",
        "\n",
        "print(f\"\\nüî¢ Missing Values:\")\n",
        "missing = business_df.isnull().sum()\n",
        "missing_pct = (missing / len(business_df) * 100).round(2)\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing,\n",
        "    'Percentage': missing_pct\n",
        "})\n",
        "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
        "if len(missing_df) > 0:\n",
        "    display(missing_df.head(10))\n",
        "else:\n",
        "    print(\"  ‚úÖ No missing values found!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 6: Initial Visualizations\n",
        "print(\"üìä CREATING INITIAL VISUALIZATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Find status column (common names)\n",
        "status_col = None\n",
        "for col in ['location_account', 'business_status', 'status', 'account_status']:\n",
        "    if col in business_df.columns:\n",
        "        status_col = col\n",
        "        break\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Business status distribution\n",
        "if status_col:\n",
        "    status_counts = business_df[status_col].value_counts()\n",
        "    axes[0].pie(status_counts.values, labels=status_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "    axes[0].set_title('Business Status Distribution', fontsize=14, fontweight='bold')\n",
        "else:\n",
        "    axes[0].text(0.5, 0.5, 'Status column not found', ha='center', va='center')\n",
        "    axes[0].set_title('Business Status Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Sample histogram (if we have date columns, we'll calculate age later)\n",
        "axes[1].hist(range(100), bins=20, color='#2ecc71', alpha=0.7)\n",
        "axes[1].set_title('Placeholder: Business Age Distribution', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Business Age (years)')\n",
        "axes[1].set_ylabel('Number of Businesses')\n",
        "axes[1].text(50, 5, 'Will be updated after data cleaning', ha='center', style='italic')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Visualizations created!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 7: Missing Data Analysis\n",
        "print(\"üîç ANALYZING MISSING DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "missing_analysis = business_df.isnull().sum()\n",
        "missing_pct = (missing_analysis / len(business_df) * 100).round(2)\n",
        "missing_df = pd.DataFrame({\n",
        "    'Column': missing_analysis.index,\n",
        "    'Missing Count': missing_analysis.values,\n",
        "    'Percentage': missing_pct.values\n",
        "})\n",
        "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
        "\n",
        "if len(missing_df) > 0:\n",
        "    print(f\"üìä Found {len(missing_df)} columns with missing values\")\n",
        "    \n",
        "    # Visualization\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    top_missing = missing_df.head(15)\n",
        "    plt.barh(range(len(top_missing)), top_missing['Percentage'].values, color='#e74c3c')\n",
        "    plt.yticks(range(len(top_missing)), top_missing['Column'].values)\n",
        "    plt.xlabel('Missing Percentage (%)', fontsize=12)\n",
        "    plt.title('Top 15 Columns with Missing Values', fontsize=14, fontweight='bold')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nüìã Top 10 columns with missing values:\")\n",
        "    display(missing_df.head(10))\n",
        "else:\n",
        "    print(\"‚úÖ No missing values found in the dataset!\")\n",
        "\n",
        "print(f\"\\nüìä Overall data completeness: {(1 - missing_df['Percentage'].sum() / (len(business_df.columns) * 100)) * 100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Data Cleaning\n",
        "\n",
        "Now we'll clean and standardize the business data, parse dates, calculate business age, and create our target variable (closed vs active)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 8: Clean Business Data\n",
        "print(\"üßπ CLEANING BUSINESS DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create a copy for cleaning\n",
        "business_clean = business_df.copy()\n",
        "\n",
        "# Standardize column names\n",
        "business_clean.columns = business_clean.columns.str.lower().str.replace(' ', '_').str.replace('-', '_')\n",
        "\n",
        "print(f\"üìã Standardized {len(business_clean.columns)} column names\")\n",
        "\n",
        "# Find date columns (common variations)\n",
        "date_start_col = None\n",
        "date_end_col = None\n",
        "\n",
        "for col in business_clean.columns:\n",
        "    col_lower = col.lower()\n",
        "    if any(x in col_lower for x in ['start', 'open', 'begin', 'registration']):\n",
        "        if 'date' in col_lower:\n",
        "            date_start_col = col\n",
        "    if any(x in col_lower for x in ['end', 'close', 'termination', 'expir']):\n",
        "        if 'date' in col_lower:\n",
        "            date_end_col = col\n",
        "\n",
        "print(f\"üìÖ Start date column: {date_start_col}\")\n",
        "print(f\"üìÖ End date column: {date_end_col}\")\n",
        "\n",
        "# Parse dates\n",
        "if date_start_col:\n",
        "    business_clean['business_start_date'] = pd.to_datetime(\n",
        "        business_clean[date_start_col], errors='coerce', infer_datetime_format=True\n",
        "    )\n",
        "    print(f\"‚úÖ Parsed start dates: {business_clean['business_start_date'].notna().sum():,} valid\")\n",
        "else:\n",
        "    # Create dummy dates if column not found\n",
        "    business_clean['business_start_date'] = pd.to_datetime('2020-01-01')\n",
        "    print(\"‚ö†Ô∏è Start date column not found, using default\")\n",
        "\n",
        "if date_end_col:\n",
        "    business_clean['business_end_date'] = pd.to_datetime(\n",
        "        business_clean[date_end_col], errors='coerce', infer_datetime_format=True\n",
        "    )\n",
        "    print(f\"‚úÖ Parsed end dates: {business_clean['business_end_date'].notna().sum():,} valid\")\n",
        "else:\n",
        "    business_clean['business_end_date'] = pd.NaT\n",
        "    print(\"‚ö†Ô∏è End date column not found, assuming all active\")\n",
        "\n",
        "# Calculate business age in years\n",
        "today = datetime.now()\n",
        "business_clean['business_age_years'] = (\n",
        "    (today - business_clean['business_start_date']).dt.days / 365.25\n",
        ").round(2)\n",
        "business_clean['business_age_years'] = business_clean['business_age_years'].clip(lower=0)\n",
        "\n",
        "# Create target variable: closed = 1 if business_end_date exists, 0 otherwise\n",
        "business_clean['closed'] = (~business_clean['business_end_date'].isna()).astype(int)\n",
        "\n",
        "print(f\"\\n‚úÖ Cleaning complete!\")\n",
        "print(f\"üìä Active businesses: {(business_clean['closed']==0).sum():,}\")\n",
        "print(f\"üìä Closed businesses: {(business_clean['closed']==1).sum():,}\")\n",
        "print(f\"üìä Closure rate: {(business_clean['closed']==1).mean()*100:.2f}%\")\n",
        "\n",
        "# Save cleaned data\n",
        "output_file = PROCESSED_DATA_DIR / 'businesses_clean.csv'\n",
        "business_clean.to_csv(output_file, index=False)\n",
        "print(f\"üíæ Saved to {output_file}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 9: Clean Violations Data\n",
        "print(\"üö® PROCESSING VIOLATIONS DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Download or load violations data\n",
        "violations_file = RAW_DATA_DIR / 'code_violations.csv'\n",
        "if violations_file.exists():\n",
        "    print(f\"üìÇ Loading from existing file: {violations_file}\")\n",
        "    violations_df = pd.read_csv(violations_file)\n",
        "else:\n",
        "    print(\"üåê Downloading from SF.gov API...\")\n",
        "    violations_df = download_sf_data(\n",
        "        DATASETS['code_violations'],\n",
        "        output_path=RAW_DATA_DIR / 'code_violations.json'\n",
        "    )\n",
        "    if len(violations_df) > 0:\n",
        "        violations_df.to_csv(violations_file, index=False)\n",
        "        print(f\"üíæ Saved to {violations_file}\")\n",
        "\n",
        "print(f\"üìä Loaded {len(violations_df):,} violation records\")\n",
        "\n",
        "# Standardize column names\n",
        "violations_df.columns = violations_df.columns.str.lower().str.replace(' ', '_').str.replace('-', '_')\n",
        "\n",
        "# Find address column\n",
        "address_col = None\n",
        "for col in violations_df.columns:\n",
        "    if 'address' in col.lower() or 'location' in col.lower():\n",
        "        address_col = col\n",
        "        break\n",
        "\n",
        "if address_col and len(violations_df) > 0:\n",
        "    print(f\"üìç Using address column: {address_col}\")\n",
        "    \n",
        "    # Aggregate violations by address\n",
        "    violations_agg = violations_df.groupby(address_col).size().reset_index(name='total_violations')\n",
        "    violations_agg.columns = ['address', 'total_violations']\n",
        "    \n",
        "    print(f\"‚úÖ Aggregated to {len(violations_agg):,} unique addresses\")\n",
        "    print(f\"üìä Total violations: {violations_agg['total_violations'].sum():,}\")\n",
        "    print(f\"üìä Average violations per address: {violations_agg['total_violations'].mean():.2f}\")\n",
        "    \n",
        "    # Save\n",
        "    output_file = PROCESSED_DATA_DIR / 'violations_clean.csv'\n",
        "    violations_agg.to_csv(output_file, index=False)\n",
        "    print(f\"üíæ Saved to {output_file}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Address column not found, creating empty violations dataset\")\n",
        "    violations_agg = pd.DataFrame(columns=['address', 'total_violations'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 10: Clean Permits Data\n",
        "print(\"üèóÔ∏è PROCESSING PERMITS DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Download or load permits data\n",
        "permits_file = RAW_DATA_DIR / 'building_permits.csv'\n",
        "if permits_file.exists():\n",
        "    print(f\"üìÇ Loading from existing file: {permits_file}\")\n",
        "    permits_df = pd.read_csv(permits_file)\n",
        "else:\n",
        "    print(\"üåê Downloading from SF.gov API...\")\n",
        "    permits_df = download_sf_data(\n",
        "        DATASETS['building_permits'],\n",
        "        output_path=RAW_DATA_DIR / 'building_permits.json'\n",
        "    )\n",
        "    if len(permits_df) > 0:\n",
        "        permits_df.to_csv(permits_file, index=False)\n",
        "        print(f\"üíæ Saved to {permits_file}\")\n",
        "\n",
        "print(f\"üìä Loaded {len(permits_df):,} permit records\")\n",
        "\n",
        "# Standardize column names\n",
        "permits_df.columns = permits_df.columns.str.lower().str.replace(' ', '_').str.replace('-', '_')\n",
        "\n",
        "# Find address and status columns\n",
        "address_col = None\n",
        "status_col = None\n",
        "\n",
        "for col in permits_df.columns:\n",
        "    if 'address' in col.lower() or 'location' in col.lower():\n",
        "        address_col = col\n",
        "    if 'status' in col.lower() or 'permit_status' in col.lower():\n",
        "        status_col = col\n",
        "\n",
        "if address_col and len(permits_df) > 0:\n",
        "    print(f\"üìç Using address column: {address_col}\")\n",
        "    print(f\"üìã Status column: {status_col if status_col else 'Not found'}\")\n",
        "    \n",
        "    # Aggregate permits by address\n",
        "    permits_agg = permits_df.groupby(address_col).agg({\n",
        "        address_col: 'count'  # Total permits\n",
        "    }).rename(columns={address_col: 'total_permits'}).reset_index()\n",
        "    permits_agg.columns = ['address', 'total_permits']\n",
        "    \n",
        "    # Count issued permits if status column exists\n",
        "    if status_col:\n",
        "        issued_permits = permits_df[\n",
        "            permits_df[status_col].str.contains('issued|approved|complete', case=False, na=False)\n",
        "        ].groupby(address_col).size().reset_index(name='issued_permits')\n",
        "        issued_permits.columns = ['address', 'issued_permits']\n",
        "        permits_agg = permits_agg.merge(issued_permits, on='address', how='left')\n",
        "        permits_agg['issued_permits'] = permits_agg['issued_permits'].fillna(0).astype(int)\n",
        "    else:\n",
        "        permits_agg['issued_permits'] = permits_agg['total_permits']  # Assume all issued\n",
        "    \n",
        "    print(f\"‚úÖ Aggregated to {len(permits_agg):,} unique addresses\")\n",
        "    print(f\"üìä Total permits: {permits_agg['total_permits'].sum():,}\")\n",
        "    print(f\"üìä Issued permits: {permits_agg['issued_permits'].sum():,}\")\n",
        "    \n",
        "    # Save\n",
        "    output_file = PROCESSED_DATA_DIR / 'permits_clean.csv'\n",
        "    permits_agg.to_csv(output_file, index=False)\n",
        "    print(f\"üíæ Saved to {output_file}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Address column not found, creating empty permits dataset\")\n",
        "    permits_agg = pd.DataFrame(columns=['address', 'total_permits', 'issued_permits'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 11: Merge Datasets\n",
        "print(\"üîó MERGING DATASETS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Find address column in business data\n",
        "business_address_col = None\n",
        "for col in business_clean.columns:\n",
        "    if 'address' in col.lower() and 'mail' not in col.lower():\n",
        "        business_address_col = col\n",
        "        break\n",
        "\n",
        "if not business_address_col:\n",
        "    # Try other location columns\n",
        "    for col in business_clean.columns:\n",
        "        if any(x in col.lower() for x in ['location', 'street', 'addr']):\n",
        "            business_address_col = col\n",
        "            break\n",
        "\n",
        "print(f\"üìç Business address column: {business_address_col}\")\n",
        "\n",
        "# Merge violations\n",
        "if business_address_col and len(violations_agg) > 0:\n",
        "    master_df = business_clean.merge(\n",
        "        violations_agg,\n",
        "        left_on=business_address_col,\n",
        "        right_on='address',\n",
        "        how='left'\n",
        "    )\n",
        "    master_df['total_violations'] = master_df['total_violations'].fillna(0).astype(int)\n",
        "    print(f\"‚úÖ Merged violations: {master_df['total_violations'].sum():,} total violations\")\n",
        "else:\n",
        "    master_df = business_clean.copy()\n",
        "    master_df['total_violations'] = 0\n",
        "    print(\"‚ö†Ô∏è Could not merge violations data\")\n",
        "\n",
        "# Merge permits\n",
        "if business_address_col and len(permits_agg) > 0:\n",
        "    master_df = master_df.merge(\n",
        "        permits_agg,\n",
        "        left_on=business_address_col,\n",
        "        right_on='address',\n",
        "        how='left',\n",
        "        suffixes=('', '_permits')\n",
        "    )\n",
        "    master_df['total_permits'] = master_df['total_permits'].fillna(0).astype(int)\n",
        "    master_df['issued_permits'] = master_df['issued_permits'].fillna(0).astype(int)\n",
        "    print(f\"‚úÖ Merged permits: {master_df['total_permits'].sum():,} total permits\")\n",
        "else:\n",
        "    master_df['total_permits'] = 0\n",
        "    master_df['issued_permits'] = 0\n",
        "    print(\"‚ö†Ô∏è Could not merge permits data\")\n",
        "\n",
        "print(f\"\\nüìä Master dataset shape: {master_df.shape}\")\n",
        "print(f\"üìä Records with violations: {(master_df['total_violations'] > 0).sum():,}\")\n",
        "print(f\"üìä Records with permits: {(master_df['total_permits'] > 0).sum():,}\")\n",
        "\n",
        "# Save merged data\n",
        "output_file = PROCESSED_DATA_DIR / 'master_business_data.csv'\n",
        "master_df.to_csv(output_file, index=False)\n",
        "print(f\"üíæ Saved to {output_file}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 12: Calculate Derived Features\n",
        "print(\"‚öôÔ∏è CREATING DERIVED FEATURES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Violation rate: violations per year of business age\n",
        "# Higher rate = more problems per year = higher risk\n",
        "master_df['violation_rate'] = np.where(\n",
        "    master_df['business_age_years'] > 0,\n",
        "    master_df['total_violations'] / master_df['business_age_years'],\n",
        "    0\n",
        ")\n",
        "master_df['violation_rate'] = master_df['violation_rate'].fillna(0).round(3)\n",
        "\n",
        "# Permit compliance rate: issued permits / total permits\n",
        "# Lower rate = more rejected permits = potential compliance issues\n",
        "master_df['permit_compliance_rate'] = np.where(\n",
        "    master_df['total_permits'] > 0,\n",
        "    master_df['issued_permits'] / master_df['total_permits'],\n",
        "    1.0  # No permits = 100% compliance (no issues)\n",
        ")\n",
        "master_df['permit_compliance_rate'] = master_df['permit_compliance_rate'].fillna(1.0).round(3)\n",
        "\n",
        "# Recent violation flag: violations in last 6 months (if we have violation dates)\n",
        "# For now, we'll use a proxy: businesses with violations and age < 1 year\n",
        "master_df['recent_violation'] = (\n",
        "    (master_df['total_violations'] > 0) & \n",
        "    (master_df['business_age_years'] < 1)\n",
        ").astype(int)\n",
        "\n",
        "# High-risk industry flag: restaurants, bars, retail (common failure industries)\n",
        "# Find industry/NAICS column\n",
        "industry_col = None\n",
        "for col in master_df.columns:\n",
        "    if any(x in col.lower() for x in ['naics', 'industry', 'business_type', 'category']):\n",
        "        industry_col = col\n",
        "        break\n",
        "\n",
        "if industry_col:\n",
        "    high_risk_keywords = ['restaurant', 'bar', 'retail', 'food', 'cafe', 'store']\n",
        "    master_df['high_risk_industry'] = master_df[industry_col].astype(str).str.lower().apply(\n",
        "        lambda x: 1 if any(keyword in x for keyword in high_risk_keywords) else 0\n",
        "    )\n",
        "    print(f\"‚úÖ High-risk industry flag: {(master_df['high_risk_industry']==1).sum():,} businesses\")\n",
        "else:\n",
        "    master_df['high_risk_industry'] = 0\n",
        "    print(\"‚ö†Ô∏è Industry column not found, setting all to 0\")\n",
        "\n",
        "print(f\"\\n‚úÖ Feature engineering complete!\")\n",
        "print(f\"üìä Features created:\")\n",
        "print(f\"  - violation_rate: {master_df['violation_rate'].describe()['mean']:.3f} avg\")\n",
        "print(f\"  - permit_compliance_rate: {master_df['permit_compliance_rate'].describe()['mean']:.3f} avg\")\n",
        "print(f\"  - recent_violation: {(master_df['recent_violation']==1).sum():,} businesses\")\n",
        "print(f\"  - high_risk_industry: {(master_df['high_risk_industry']==1).sum():,} businesses\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 13: Feature Analysis\n",
        "print(\"üìä ANALYZING FEATURES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Summary statistics by closure status\n",
        "feature_cols = [\n",
        "    'business_age_years', 'total_violations', 'violation_rate',\n",
        "    'total_permits', 'issued_permits', 'permit_compliance_rate',\n",
        "    'recent_violation', 'high_risk_industry'\n",
        "]\n",
        "\n",
        "print(\"üìà Summary Statistics by Closure Status:\")\n",
        "summary_stats = master_df.groupby('closed')[feature_cols].agg(['mean', 'std', 'median'])\n",
        "display(summary_stats)\n",
        "\n",
        "# Correlation heatmap\n",
        "print(\"\\nüîó Feature Correlation Matrix:\")\n",
        "correlation_cols = feature_cols + ['closed']\n",
        "corr_matrix = master_df[correlation_cols].corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Feature distributions: Active vs Closed\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, col in enumerate(feature_cols):\n",
        "    if col in master_df.columns:\n",
        "        active_data = master_df[master_df['closed'] == 0][col]\n",
        "        closed_data = master_df[master_df['closed'] == 1][col]\n",
        "        \n",
        "        axes[idx].hist(active_data, bins=30, alpha=0.6, label='Active', color='#2ecc71', density=True)\n",
        "        axes[idx].hist(closed_data, bins=30, alpha=0.6, label='Closed', color='#e74c3c', density=True)\n",
        "        axes[idx].set_title(col.replace('_', ' ').title(), fontweight='bold')\n",
        "        axes[idx].set_xlabel('Value')\n",
        "        axes[idx].set_ylabel('Density')\n",
        "        axes[idx].legend()\n",
        "        axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Feature Distributions: Active vs Closed Businesses', \n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Feature analysis complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Model Preparation\n",
        "\n",
        "We'll now prepare the training data by selecting features, checking class distribution, and splitting into train/test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 14: Prepare Training Data\n",
        "print(\"üéØ PREPARING TRAINING DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Select feature columns\n",
        "feature_columns = [\n",
        "    'business_age_years',\n",
        "    'total_violations',\n",
        "    'violation_rate',\n",
        "    'total_permits',\n",
        "    'issued_permits',\n",
        "    'permit_compliance_rate',\n",
        "    'recent_violation',\n",
        "    'high_risk_industry'\n",
        "]\n",
        "\n",
        "# Check which features exist\n",
        "available_features = [col for col in feature_columns if col in master_df.columns]\n",
        "missing_features = [col for col in feature_columns if col not in master_df.columns]\n",
        "\n",
        "print(f\"‚úÖ Available features: {len(available_features)}/{len(feature_columns)}\")\n",
        "if missing_features:\n",
        "    print(f\"‚ö†Ô∏è Missing features: {missing_features}\")\n",
        "\n",
        "# Create feature matrix X and target y\n",
        "X = master_df[available_features].copy()\n",
        "y = master_df['closed'].copy()\n",
        "\n",
        "# Remove rows with any NaN values\n",
        "valid_mask = ~X.isnull().any(axis=1)\n",
        "X = X[valid_mask]\n",
        "y = y[valid_mask]\n",
        "\n",
        "print(f\"\\nüìä Dataset after cleaning:\")\n",
        "print(f\"  - Total records: {len(X):,}\")\n",
        "print(f\"  - Features: {len(available_features)}\")\n",
        "print(f\"  - Active businesses (y=0): {(y==0).sum():,} ({(y==0).mean()*100:.2f}%)\")\n",
        "print(f\"  - Closed businesses (y=1): {(y==1).sum():,} ({(y==1).mean()*100:.2f}%)\")\n",
        "\n",
        "# Check class imbalance\n",
        "class_ratio = (y==1).sum() / (y==0).sum() if (y==0).sum() > 0 else 0\n",
        "if class_ratio < 0.1 or class_ratio > 10:\n",
        "    print(f\"\\n‚ö†Ô∏è Class imbalance detected: {class_ratio:.3f}\")\n",
        "    print(\"   Consider using class_weight='balanced' or SMOTE\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Class balance is reasonable: {class_ratio:.3f}\")\n",
        "\n",
        "print(f\"\\nüìã Feature summary:\")\n",
        "display(X.describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 15: Train/Test Split\n",
        "print(\"‚úÇÔ∏è SPLITTING DATA INTO TRAIN/TEST SETS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=TEST_SIZE,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y  # Maintain class distribution\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Split complete!\")\n",
        "print(f\"\\nüìä Training Set:\")\n",
        "print(f\"  - Size: {len(X_train):,} samples\")\n",
        "print(f\"  - Active: {(y_train==0).sum():,} ({(y_train==0).mean()*100:.2f}%)\")\n",
        "print(f\"  - Closed: {(y_train==1).sum():,} ({(y_train==1).mean()*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nüìä Test Set:\")\n",
        "print(f\"  - Size: {len(X_test):,} samples\")\n",
        "print(f\"  - Active: {(y_test==0).sum():,} ({(y_test==0).mean()*100:.2f}%)\")\n",
        "print(f\"  - Closed: {(y_test==1).sum():,} ({(y_test==1).mean()*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\n‚úÖ Ready for model training!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Model Training\n",
        "\n",
        "We'll train a Gradient Boosting Classifier, which is excellent for tabular data and can capture complex feature interactions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 16: Train Gradient Boosting Model\n",
        "print(\"üöÄ TRAINING GRADIENT BOOSTING MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize model with specified parameters\n",
        "model = GradientBoostingClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    min_samples_split=20,\n",
        "    min_samples_leaf=10,\n",
        "    subsample=0.8,\n",
        "    random_state=RANDOM_STATE,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"üìä Model Configuration:\")\n",
        "print(f\"  - n_estimators: {model.n_estimators}\")\n",
        "print(f\"  - learning_rate: {model.learning_rate}\")\n",
        "print(f\"  - max_depth: {model.max_depth}\")\n",
        "print(f\"  - min_samples_split: {model.min_samples_split}\")\n",
        "print(f\"  - min_samples_leaf: {model.min_samples_leaf}\")\n",
        "print(f\"  - subsample: {model.subsample}\")\n",
        "\n",
        "# Train the model\n",
        "print(f\"\\nüîÑ Training on {len(X_train):,} samples...\")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracies\n",
        "train_accuracy = (y_train_pred == y_train).mean()\n",
        "test_accuracy = (y_test_pred == y_test).mean()\n",
        "\n",
        "print(f\"\\n‚úÖ Training complete!\")\n",
        "print(f\"üìä Training Accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"üìä Test Accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 18: Feature Importance\n",
        "print(\"üéØ ANALYZING FEATURE IMPORTANCE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Extract feature importances\n",
        "importances = model.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'feature': available_features,\n",
        "    'importance': importances\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"üìä Feature Importance Rankings:\")\n",
        "display(feature_importance_df)\n",
        "\n",
        "print(f\"\\nüèÜ Top 5 Most Important Features:\")\n",
        "for idx, row in feature_importance_df.head(5).iterrows():\n",
        "    print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = ['#2ecc71' if x > 0.1 else '#f39c12' if x > 0.05 else '#e74c3c' \n",
        "          for x in feature_importance_df['importance']]\n",
        "plt.barh(range(len(feature_importance_df)), feature_importance_df['importance'], color=colors)\n",
        "plt.yticks(range(len(feature_importance_df)), feature_importance_df['feature'])\n",
        "plt.xlabel('Importance Score', fontsize=12)\n",
        "plt.title('Feature Importance (Gradient Boosting)', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(True, alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Feature importance analysis complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Visualizations\n",
        "\n",
        "Let's create comprehensive visualizations to understand model performance and predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 19: Confusion Matrix Visualization\n",
        "print(\"üìä CREATING CONFUSION MATRIX VISUALIZATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Training set confusion matrix\n",
        "sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=['Active', 'Closed'], yticklabels=['Active', 'Closed'])\n",
        "axes[0].set_title('Confusion Matrix - Training Set', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('True Label', fontsize=10)\n",
        "axes[0].set_xlabel('Predicted Label', fontsize=10)\n",
        "\n",
        "# Test set confusion matrix\n",
        "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Reds', ax=axes[1],\n",
        "            xticklabels=['Active', 'Closed'], yticklabels=['Active', 'Closed'])\n",
        "axes[1].set_title('Confusion Matrix - Test Set', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('True Label', fontsize=10)\n",
        "axes[1].set_xlabel('Predicted Label', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate metrics\n",
        "tn_train, fp_train, fn_train, tp_train = cm_train.ravel()\n",
        "tn_test, fp_test, fn_test, tp_test = cm_test.ravel()\n",
        "\n",
        "print(\"üìä Training Set Metrics:\")\n",
        "print(f\"  - Precision: {tp_train/(tp_train+fp_train):.4f}\" if (tp_train+fp_train) > 0 else \"  - Precision: N/A\")\n",
        "print(f\"  - Recall: {tp_train/(tp_train+fn_train):.4f}\" if (tp_train+fn_train) > 0 else \"  - Recall: N/A\")\n",
        "print(f\"  - F1-Score: {2*tp_train/(2*tp_train+fp_train+fn_train):.4f}\" if (2*tp_train+fp_train+fn_train) > 0 else \"  - F1-Score: N/A\")\n",
        "\n",
        "print(\"\\nüìä Test Set Metrics:\")\n",
        "print(f\"  - Precision: {tp_test/(tp_test+fp_test):.4f}\" if (tp_test+fp_test) > 0 else \"  - Precision: N/A\")\n",
        "print(f\"  - Recall: {tp_test/(tp_test+fn_test):.4f}\" if (tp_test+fn_test) > 0 else \"  - Recall: N/A\")\n",
        "print(f\"  - F1-Score: {2*tp_test/(2*tp_test+fp_test+fn_test):.4f}\" if (2*tp_test+fp_test+fn_test) > 0 else \"  - F1-Score: N/A\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 20: ROC Curve\n",
        "print(\"üìà CREATING ROC CURVE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr, tpr, color='#2ecc71', lw=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='#95a5a6', lw=2, linestyle='--', label='Baseline (AUC = 0.5000)')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc=\"lower right\", fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"‚úÖ ROC-AUC Score: {roc_auc:.4f}\")\n",
        "if roc_auc > 0.9:\n",
        "    print(\"  üéâ Outstanding discrimination ability!\")\n",
        "elif roc_auc > 0.8:\n",
        "    print(\"  ‚úÖ Excellent discrimination ability\")\n",
        "elif roc_auc > 0.7:\n",
        "    print(\"  ‚úÖ Good discrimination ability\")\n",
        "else:\n",
        "    print(\"  ‚ö†Ô∏è Model may benefit from feature engineering or different algorithm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 21: Precision-Recall Curve\n",
        "print(\"üìä CREATING PRECISION-RECALL CURVE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Calculate precision-recall curve\n",
        "precision_vals, recall_vals, pr_thresholds = precision_recall_curve(y_test, y_test_proba)\n",
        "avg_precision = average_precision_score(y_test, y_test_proba)\n",
        "\n",
        "# Plot PR curve\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(recall_vals, precision_vals, color='#e74c3c', lw=2, \n",
        "         label=f'PR Curve (AP = {avg_precision:.4f})')\n",
        "plt.xlabel('Recall', fontsize=12)\n",
        "plt.ylabel('Precision', fontsize=12)\n",
        "plt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc=\"lower left\", fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find optimal threshold (F1 score)\n",
        "f1_scores = 2 * (precision_vals * recall_vals) / (precision_vals + recall_vals + 1e-10)\n",
        "optimal_idx = np.argmax(f1_scores)\n",
        "optimal_threshold = pr_thresholds[optimal_idx] if optimal_idx < len(pr_thresholds) else 0.5\n",
        "optimal_precision = precision_vals[optimal_idx]\n",
        "optimal_recall = recall_vals[optimal_idx]\n",
        "optimal_f1 = f1_scores[optimal_idx]\n",
        "\n",
        "print(f\"‚úÖ Average Precision: {avg_precision:.4f}\")\n",
        "print(f\"\\nüéØ Optimal Threshold (F1-maximizing):\")\n",
        "print(f\"  - Threshold: {optimal_threshold:.4f}\")\n",
        "print(f\"  - Precision: {optimal_precision:.4f}\")\n",
        "print(f\"  - Recall: {optimal_recall:.4f}\")\n",
        "print(f\"  - F1-Score: {optimal_f1:.4f}\")\n",
        "\n",
        "# Mark optimal point on plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(recall_vals, precision_vals, color='#e74c3c', lw=2, \n",
        "         label=f'PR Curve (AP = {avg_precision:.4f})')\n",
        "plt.plot(optimal_recall, optimal_precision, 'ro', markersize=12, \n",
        "         label=f'Optimal Point (F1={optimal_f1:.3f})')\n",
        "plt.xlabel('Recall', fontsize=12)\n",
        "plt.ylabel('Precision', fontsize=12)\n",
        "plt.title('Precision-Recall Curve with Optimal Threshold', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc=\"lower left\", fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: Model Deployment\n",
        "\n",
        "We'll save the trained model and create a prediction function for real-world use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 22: Save Model\n",
        "print(\"üíæ SAVING MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create model package with metadata\n",
        "model_package = {\n",
        "    'model': model,\n",
        "    'feature_columns': available_features,\n",
        "    'metrics': {\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'roc_auc': roc_auc,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1\n",
        "    },\n",
        "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'n_samples_train': len(X_train),\n",
        "    'n_samples_test': len(X_test),\n",
        "    'class_distribution': {\n",
        "        'active': int((y==0).sum()),\n",
        "        'closed': int((y==1).sum())\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save model\n",
        "model_path = MODELS_DIR / 'risk_model.joblib'\n",
        "joblib.dump(model_package, model_path)\n",
        "\n",
        "print(f\"‚úÖ Model saved to: {model_path}\")\n",
        "print(f\"\\nüìä Model Metadata:\")\n",
        "print(f\"  - Training Date: {model_package['training_date']}\")\n",
        "print(f\"  - Features: {len(model_package['feature_columns'])}\")\n",
        "print(f\"  - Test Accuracy: {model_package['metrics']['test_accuracy']:.4f}\")\n",
        "print(f\"  - ROC-AUC: {model_package['metrics']['roc_auc']:.4f}\")\n",
        "print(f\"  - Training Samples: {model_package['n_samples_train']:,}\")\n",
        "print(f\"  - Test Samples: {model_package['n_samples_test']:,}\")\n",
        "\n",
        "print(f\"\\nüíæ Model saved successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 23: Load and Test Model\n",
        "print(\"üîÑ LOADING AND TESTING SAVED MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load model\n",
        "loaded_package = joblib.load(model_path)\n",
        "loaded_model = loaded_package['model']\n",
        "loaded_features = loaded_package['feature_columns']\n",
        "\n",
        "print(f\"‚úÖ Model loaded successfully!\")\n",
        "print(f\"üìä Loaded {len(loaded_features)} features\")\n",
        "print(f\"üìÖ Training date: {loaded_package['training_date']}\")\n",
        "\n",
        "# Verify model works\n",
        "test_sample = X_test.iloc[:5]\n",
        "predictions = loaded_model.predict(test_sample)\n",
        "probabilities = loaded_model.predict_proba(test_sample)[:, 1]\n",
        "\n",
        "print(f\"\\nüß™ Testing on 5 samples:\")\n",
        "for idx, (pred, prob) in enumerate(zip(predictions, probabilities)):\n",
        "    status = \"Closed\" if pred == 1 else \"Active\"\n",
        "    print(f\"  Sample {idx+1}: {status} (probability: {prob:.4f})\")\n",
        "\n",
        "print(f\"\\n‚úÖ Model verification complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 24: Prediction Function\n",
        "print(\"üîÆ CREATING PREDICTION FUNCTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def predict_business_risk(business_data: dict, model_package: dict = None) -> dict:\n",
        "    \"\"\"\n",
        "    Predict business closure risk for a single business\n",
        "    \n",
        "    Args:\n",
        "        business_data: Dictionary with business features\n",
        "        model_package: Loaded model package (if None, loads from disk)\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with risk_score, risk_level, and color\n",
        "    \"\"\"\n",
        "    if model_package is None:\n",
        "        model_package = joblib.load(MODELS_DIR / 'risk_model.joblib')\n",
        "    \n",
        "    model = model_package['model']\n",
        "    feature_columns = model_package['feature_columns']\n",
        "    \n",
        "    # Create feature vector\n",
        "    feature_vector = pd.DataFrame([{\n",
        "        col: business_data.get(col, 0) for col in feature_columns\n",
        "    }])\n",
        "    \n",
        "    # Ensure all features are present\n",
        "    for col in feature_columns:\n",
        "        if col not in feature_vector.columns:\n",
        "            feature_vector[col] = 0\n",
        "    \n",
        "    # Reorder columns to match training\n",
        "    feature_vector = feature_vector[feature_columns]\n",
        "    \n",
        "    # Predict\n",
        "    risk_score = model.predict_proba(feature_vector)[0, 1]  # Probability of closure\n",
        "    \n",
        "    # Determine risk level\n",
        "    if risk_score >= 0.7:\n",
        "        risk_level = \"High\"\n",
        "        color = \"#e74c3c\"  # Red\n",
        "    elif risk_score >= 0.4:\n",
        "        risk_level = \"Medium\"\n",
        "        color = \"#f39c12\"  # Orange\n",
        "    else:\n",
        "        risk_level = \"Low\"\n",
        "        color = \"#2ecc71\"  # Green\n",
        "    \n",
        "    return {\n",
        "        'risk_score': float(risk_score),\n",
        "        'risk_level': risk_level,\n",
        "        'color': color\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Prediction function created!\")\n",
        "print(\"\\nüìù Function signature:\")\n",
        "print(\"  predict_business_risk(business_data: dict) -> dict\")\n",
        "print(\"\\nüìã Required features:\")\n",
        "for col in available_features:\n",
        "    print(f\"  - {col}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 25: Example Predictions\n",
        "print(\"üéØ GENERATING EXAMPLE PREDICTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create 3 sample businesses\n",
        "sample_businesses = [\n",
        "    {\n",
        "        'name': 'High Risk Restaurant',\n",
        "        'business_age_years': 0.5,\n",
        "        'total_violations': 5,\n",
        "        'violation_rate': 10.0,\n",
        "        'total_permits': 2,\n",
        "        'issued_permits': 1,\n",
        "        'permit_compliance_rate': 0.5,\n",
        "        'recent_violation': 1,\n",
        "        'high_risk_industry': 1\n",
        "    },\n",
        "    {\n",
        "        'name': 'Medium Risk Retail Store',\n",
        "        'business_age_years': 3.0,\n",
        "        'total_violations': 2,\n",
        "        'violation_rate': 0.67,\n",
        "        'total_permits': 5,\n",
        "        'issued_permits': 4,\n",
        "        'permit_compliance_rate': 0.8,\n",
        "        'recent_violation': 0,\n",
        "        'high_risk_industry': 1\n",
        "    },\n",
        "    {\n",
        "        'name': 'Low Risk Established Business',\n",
        "        'business_age_years': 10.0,\n",
        "        'total_violations': 0,\n",
        "        'violation_rate': 0.0,\n",
        "        'total_permits': 8,\n",
        "        'issued_permits': 8,\n",
        "        'permit_compliance_rate': 1.0,\n",
        "        'recent_violation': 0,\n",
        "        'high_risk_industry': 0\n",
        "    }\n",
        "]\n",
        "\n",
        "# Generate predictions\n",
        "results = []\n",
        "for business in sample_businesses:\n",
        "    prediction = predict_business_risk(business)\n",
        "    results.append({\n",
        "        'Business Name': business['name'],\n",
        "        'Risk Score': f\"{prediction['risk_score']:.4f}\",\n",
        "        'Risk Level': prediction['risk_level'],\n",
        "        'Age (years)': business['business_age_years'],\n",
        "        'Violations': business['total_violations'],\n",
        "        'Violation Rate': business['violation_rate']\n",
        "    })\n",
        "\n",
        "# Display results\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nüìä Prediction Results:\")\n",
        "display(results_df)\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "colors_list = [predict_business_risk(b)['color'] for b in sample_businesses]\n",
        "bars = ax.barh(results_df['Business Name'], \n",
        "                [float(x) for x in results_df['Risk Score']],\n",
        "                color=colors_list, alpha=0.7)\n",
        "ax.set_xlabel('Risk Score', fontsize=12)\n",
        "ax.set_title('Example Business Risk Predictions', fontsize=14, fontweight='bold')\n",
        "ax.set_xlim([0, 1])\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Add value labels\n",
        "for i, (bar, score) in enumerate(zip(bars, results_df['Risk Score'])):\n",
        "    ax.text(float(score) + 0.02, i, f\"{float(score):.3f}\", \n",
        "            va='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Example predictions complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 9: Real-World Application\n",
        "\n",
        "Let's apply the model to screen all active businesses and identify high-risk cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 26: Batch Risk Screening\n",
        "print(\"üîç BATCH RISK SCREENING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Screen all active businesses (closed = 0)\n",
        "active_businesses = master_df[master_df['closed'] == 0].copy()\n",
        "\n",
        "print(f\"üìä Screening {len(active_businesses):,} active businesses...\")\n",
        "\n",
        "# Prepare features\n",
        "X_active = active_businesses[available_features].copy()\n",
        "valid_mask = ~X_active.isnull().any(axis=1)\n",
        "X_active = X_active[valid_mask]\n",
        "active_businesses_valid = active_businesses[valid_mask].copy()\n",
        "\n",
        "# Predict risk scores\n",
        "risk_scores = model.predict_proba(X_active)[:, 1]\n",
        "active_businesses_valid['risk_score'] = risk_scores\n",
        "\n",
        "# Determine risk levels\n",
        "active_businesses_valid['risk_level'] = active_businesses_valid['risk_score'].apply(\n",
        "    lambda x: 'High' if x >= 0.7 else 'Medium' if x >= 0.4 else 'Low'\n",
        ")\n",
        "\n",
        "# Identify top 20 highest-risk businesses\n",
        "top_risks = active_businesses_valid.nlargest(20, 'risk_score')[\n",
        "    ['risk_score', 'risk_level', 'business_age_years', 'total_violations', \n",
        "     'violation_rate', 'total_permits'] + \n",
        "    ([business_address_col] if business_address_col and business_address_col in active_businesses_valid.columns else [])\n",
        "]\n",
        "\n",
        "print(f\"\\nüö® Top 20 Highest-Risk Active Businesses:\")\n",
        "display(top_risks)\n",
        "\n",
        "# Summary statistics\n",
        "print(f\"\\nüìä Risk Distribution:\")\n",
        "risk_dist = active_businesses_valid['risk_level'].value_counts()\n",
        "print(risk_dist)\n",
        "\n",
        "print(f\"\\nüìà Risk Score Statistics:\")\n",
        "print(active_businesses_valid['risk_score'].describe())\n",
        "\n",
        "# Save results\n",
        "output_file = PROCESSED_DATA_DIR / 'active_businesses_risk_scores.csv'\n",
        "active_businesses_valid[['risk_score', 'risk_level'] + available_features].to_csv(\n",
        "    output_file, index=False\n",
        ")\n",
        "print(f\"\\nüíæ Saved risk scores to {output_file}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 27: Risk Alert Dashboard\n",
        "print(\"üìä CREATING RISK ALERT DASHBOARD\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Risk distribution histogram\n",
        "axes[0].hist(active_businesses_valid['risk_score'], bins=50, \n",
        "             color='#3498db', alpha=0.7, edgecolor='black')\n",
        "axes[0].axvline(x=0.4, color='#f39c12', linestyle='--', linewidth=2, label='Medium Threshold')\n",
        "axes[0].axvline(x=0.7, color='#e74c3c', linestyle='--', linewidth=2, label='High Threshold')\n",
        "axes[0].set_xlabel('Risk Score', fontsize=12)\n",
        "axes[0].set_ylabel('Number of Businesses', fontsize=12)\n",
        "axes[0].set_title('Risk Score Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Risk level pie chart\n",
        "risk_counts = active_businesses_valid['risk_level'].value_counts()\n",
        "colors_pie = ['#2ecc71' if x == 'Low' else '#f39c12' if x == 'Medium' else '#e74c3c' \n",
        "              for x in risk_counts.index]\n",
        "axes[1].pie(risk_counts.values, labels=risk_counts.index, autopct='%1.1f%%',\n",
        "           colors=colors_pie, startangle=90, textprops={'fontsize': 11, 'fontweight': 'bold'})\n",
        "axes[1].set_title('Risk Level Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# High-risk businesses with recommendations\n",
        "high_risk = active_businesses_valid[active_businesses_valid['risk_level'] == 'High'].copy()\n",
        "print(f\"\\nüö® HIGH-RISK BUSINESSES: {len(high_risk):,}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if len(high_risk) > 0:\n",
        "    print(f\"\\nüìã Top 10 High-Risk Businesses:\")\n",
        "    display(high_risk.nlargest(10, 'risk_score')[\n",
        "        ['risk_score', 'business_age_years', 'total_violations', \n",
        "         'violation_rate', 'total_permits']\n",
        "    ])\n",
        "    \n",
        "    print(f\"\\nüí° Recommendations for High-Risk Businesses:\")\n",
        "    print(\"  1. Review and address code violations immediately\")\n",
        "    print(\"  2. Ensure all required permits are obtained and up-to-date\")\n",
        "    print(\"  3. Consider compliance consultation services\")\n",
        "    print(\"  4. Monitor business metrics more frequently\")\n",
        "    print(\"  5. Develop risk mitigation strategies\")\n",
        "else:\n",
        "    print(\"  ‚úÖ No high-risk businesses identified!\")\n",
        "\n",
        "print(f\"\\n‚úÖ Risk alert dashboard complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 10: Summary & Next Steps\n",
        "\n",
        "Let's summarize the model performance, key findings, and potential improvements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 28: Summary\n",
        "print(\"üìã MODEL SUMMARY & KEY FINDINGS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüéØ MODEL PERFORMANCE SUMMARY:\")\n",
        "print(f\"  ‚úÖ Test Accuracy: {test_accuracy*100:.2f}%\")\n",
        "print(f\"  ‚úÖ ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"  ‚úÖ Precision: {precision:.4f}\")\n",
        "print(f\"  ‚úÖ Recall: {recall:.4f}\")\n",
        "print(f\"  ‚úÖ F1-Score: {f1:.4f}\")\n",
        "\n",
        "print(\"\\nüìä DATASET STATISTICS:\")\n",
        "print(f\"  üìà Total Businesses Analyzed: {len(master_df):,}\")\n",
        "print(f\"  ‚úÖ Active Businesses: {(master_df['closed']==0).sum():,}\")\n",
        "print(f\"  ‚ùå Closed Businesses: {(master_df['closed']==1).sum():,}\")\n",
        "print(f\"  üìä Closure Rate: {(master_df['closed']==1).mean()*100:.2f}%\")\n",
        "\n",
        "print(\"\\nüîë KEY FINDINGS:\")\n",
        "print(\"  1. Feature Importance:\")\n",
        "for idx, row in feature_importance_df.head(3).iterrows():\n",
        "    print(f\"     - {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "print(\"\\n  2. Risk Distribution (Active Businesses):\")\n",
        "if 'active_businesses_valid' in locals() and len(active_businesses_valid) > 0:\n",
        "    risk_dist = active_businesses_valid['risk_level'].value_counts()\n",
        "    for level, count in risk_dist.items():\n",
        "        pct = (count / len(active_businesses_valid)) * 100\n",
        "        print(f\"     - {level} Risk: {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "print(\"\\n  3. Business Insights:\")\n",
        "print(f\"     - Average business age: {master_df['business_age_years'].mean():.2f} years\")\n",
        "print(f\"     - Businesses with violations: {(master_df['total_violations'] > 0).sum():,}\")\n",
        "print(f\"     - Average violations per business: {master_df['total_violations'].mean():.2f}\")\n",
        "\n",
        "print(\"\\nüöÄ NEXT STEPS FOR IMPROVEMENT:\")\n",
        "print(\"  1. Feature Engineering:\")\n",
        "print(\"     - Add economic indicators (unemployment, GDP)\")\n",
        "print(\"     - Include neighborhood demographics\")\n",
        "print(\"     - Add seasonal/temporal features\")\n",
        "print(\"     - Incorporate competitor density metrics\")\n",
        "print(\"\\n  2. Model Enhancement:\")\n",
        "print(\"     - Try ensemble methods (XGBoost, LightGBM)\")\n",
        "print(\"     - Implement hyperparameter tuning\")\n",
        "print(\"     - Add cross-validation for robust evaluation\")\n",
        "print(\"     - Consider deep learning for complex patterns\")\n",
        "print(\"\\n  3. Data Improvements:\")\n",
        "print(\"     - Collect more recent data\")\n",
        "print(\"     - Add financial data (revenue, expenses)\")\n",
        "print(\"     - Include customer reviews/sentiment\")\n",
        "print(\"     - Track real-time business events\")\n",
        "print(\"\\n  4. Deployment:\")\n",
        "print(\"     - Create API endpoint for real-time predictions\")\n",
        "print(\"     - Build monitoring dashboard\")\n",
        "print(\"     - Implement model retraining pipeline\")\n",
        "print(\"     - Add explainability features (SHAP values)\")\n",
        "\n",
        "print(\"\\n‚úÖ Analysis complete! Model is ready for deployment.\")\n",
        "print(\"=\"*60)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}